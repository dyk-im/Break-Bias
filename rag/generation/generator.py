from typing import List, Optional, Tuple, Dict
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate
from app.models.schemas import ChatMessage, CommentData, SentimentResult
from rag.sentiment_analyzer import SentimentAnalyzer
from config.settings import settings
import google.generativeai as genai


class OpinionAnalysisGenerator:
    """ëŒ“ê¸€ ë¶„ì„ ë° ì—¬ë¡  ìš”ì•½ì„ ìœ„í•œ LLM ì‘ë‹µ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.sentiment_analyzer = SentimentAnalyzer()
        self.llm = self._initialize_llm()
        self.analysis_prompt_template = self._create_analysis_prompt_template()
        self.summary_prompt_template = self._create_summary_prompt_template()
    
    def _initialize_llm(self):
        """LLM ì´ˆê¸°í™” (Gemini ì‚¬ìš©)"""
        
        if not settings.gemini_api_key:
            return None
            
        try:
            return ChatGoogleGenerativeAI(
                model="gemini-2.0-flash-exp",
                google_api_key=settings.gemini_api_key,
                temperature=settings.temperature,
                max_output_tokens=settings.max_tokens
            )
        except Exception as e:
            print(f"Gemini ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def _create_analysis_prompt_template(self) -> ChatPromptTemplate:
        """ëŒ“ê¸€ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        system_message = """ë‹¹ì‹ ì€ ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ëŒ“ê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì—¬ë¡ ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”.

ë¶„ì„ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì„¸ìš”:
1. ëŒ“ê¸€ì˜ ì „ë°˜ì ì¸ ê°ì •(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
2. ì£¼ìš” ë…¼ì ê³¼ ì˜ê²¬ë“¤
3. ì°¬ì„±/ë°˜ëŒ€ ì˜ê²¬ì˜ ê·¼ê±°
4. ëŒ“ê¸€ ì‘ì„±ìë“¤ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬
5. ì—¬ë¡ ì˜ ì „ë°˜ì ì¸ ë°©í–¥ì„±

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
### ğŸ“Š ì—¬ë¡  ë¶„ì„ ìš”ì•½
[ì „ì²´ì ì¸ ì—¬ë¡  ë™í–¥ê³¼ ì£¼ìš” í¬ì¸íŠ¸]

### ğŸ’­ ì£¼ìš” ì˜ê²¬ë“¤
[ê¸ì •ì  ì˜ê²¬]
- ì˜ê²¬ 1
- ì˜ê²¬ 2

[ë¶€ì •ì  ì˜ê²¬]  
- ì˜ê²¬ 1
- ì˜ê²¬ 2

[ì¤‘ë¦½ì /ê¸°íƒ€ ì˜ê²¬]
- ì˜ê²¬ 1
- ì˜ê²¬ 2

### ğŸ¯ ê²°ë¡ 
[ì¢…í•©ì ì¸ ë¶„ì„ ê²°ê³¼]

ê´€ë ¨ ëŒ“ê¸€ë“¤:
{comments}

ê°ì • ë¶„ì„ ê²°ê³¼: {sentiment_stats}"""

        human_message = "ì§ˆë¬¸: {query}"
        
        return ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("human", human_message)
        ])
    
    def _create_summary_prompt_template(self) -> ChatPromptTemplate:
        """ê°„ë‹¨ ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"""
        system_message = """ì£¼ì–´ì§„ ëŒ“ê¸€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , 3-5ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ëŒ“ê¸€ ë‚´ìš©: {comments}
ê°ì • ë¶„ì„: {sentiment_stats}"""

        human_message = "ì§ˆë¬¸: {query}"
        
        return ChatPromptTemplate.from_messages([
            ("system", system_message),
            ("human", human_message)
        ])
    
    async def generate_opinion_analysis(
        self, 
        query: str, 
        relevant_comments: List[Dict], 
        detailed: bool = True
    ) -> Tuple[str, Dict]:
        """ì—¬ë¡  ë¶„ì„ ì‘ë‹µ ìƒì„±"""
        
        if not relevant_comments:
            return self._generate_no_data_response(query), {}
        
        # 1. ëŒ“ê¸€ì—ì„œ CommentData ì¶”ì¶œ ë˜ëŠ” êµ¬ì„±
        comment_texts = []
        for comment_dict in relevant_comments:
            content = comment_dict.get("content", "")
            metadata = comment_dict.get("metadata", {})
            comment_texts.append({
                "text": content,
                "author": metadata.get("author", "ìµëª…"),
                "like_count": metadata.get("like_count", 0),
                "video_title": metadata.get("video_title", "")
            })
        
        # 2. ê°ì • ë¶„ì„
        sentiment_stats = await self._analyze_comment_sentiments(comment_texts)
        
        # 3. ëŒ“ê¸€ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        formatted_comments = self._format_comments_for_llm(comment_texts)
        
        # 4. LLM ì‘ë‹µ ìƒì„±
        if not self.llm:
            return self._generate_dummy_analysis(query, sentiment_stats), sentiment_stats
        
        try:
            if detailed:
                prompt = self.analysis_prompt_template.format_messages(
                    query=query,
                    comments=formatted_comments,
                    sentiment_stats=self._format_sentiment_stats(sentiment_stats)
                )
            else:
                prompt = self.summary_prompt_template.format_messages(
                    query=query,
                    comments=formatted_comments,
                    sentiment_stats=self._format_sentiment_stats(sentiment_stats)
                )
            
            response = await self.llm.agenerate([prompt])
            analysis_text = response.generations[0][0].text.strip()
            
            return analysis_text, sentiment_stats
            
        except Exception as e:
            error_msg = f"ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            return error_msg, sentiment_stats
    
    async def _analyze_comment_sentiments(self, comment_texts: List[Dict]) -> Dict:
        """ëŒ“ê¸€ë“¤ì˜ ê°ì • ë¶„ì„"""
        if not comment_texts:
            return {
                "positive": 0.0,
                "negative": 0.0,
                "neutral": 1.0,
                "total_comments": 0,
                "dominant": "neutral"
            }
        
        sentiments = []
        for comment in comment_texts:
            sentiment = await self.sentiment_analyzer.analyze_single_comment(comment["text"])
            sentiments.append(sentiment)
        
        # í‰ê·  ê³„ì‚°
        avg_positive = sum(s["positive"] for s in sentiments) / len(sentiments)
        avg_negative = sum(s["negative"] for s in sentiments) / len(sentiments)
        avg_neutral = sum(s["neutral"] for s in sentiments) / len(sentiments)
        
        # ì§€ë°°ì  ê°ì •
        if avg_positive > avg_negative and avg_positive > avg_neutral:
            dominant = "positive"
        elif avg_negative > avg_positive and avg_negative > avg_neutral:
            dominant = "negative"
        else:
            dominant = "neutral"
        
        return {
            "positive": round(avg_positive, 3),
            "negative": round(avg_negative, 3),
            "neutral": round(avg_neutral, 3),
            "total_comments": len(comment_texts),
            "dominant": dominant
        }
    
    def _format_comments_for_llm(self, comment_texts: List[Dict]) -> str:
        """LLMì— ì „ë‹¬í•  ëŒ“ê¸€ í¬ë§·íŒ…"""
        formatted = []
        
        for i, comment in enumerate(comment_texts[:20], 1):  # ìµœëŒ€ 20ê°œë§Œ
            text = comment["text"][:200]  # ê° ëŒ“ê¸€ ìµœëŒ€ 200ì
            author = comment["author"]
            likes = comment["like_count"]
            
            formatted.append(f"[ëŒ“ê¸€ {i}] {author} (ğŸ‘{likes}): {text}")
        
        return "\n".join(formatted)
    
    def _format_sentiment_stats(self, sentiment_stats: Dict) -> str:
        """ê°ì • í†µê³„ í¬ë§·íŒ…"""
        return f"""
ê¸ì •: {sentiment_stats['positive']:.1%}
ë¶€ì •: {sentiment_stats['negative']:.1%}  
ì¤‘ë¦½: {sentiment_stats['neutral']:.1%}
ì „ì²´ ëŒ“ê¸€ ìˆ˜: {sentiment_stats['total_comments']}ê°œ
ì§€ë°°ì  ê°ì •: {sentiment_stats['dominant']}
"""
    
    def _generate_dummy_analysis(self, query: str, sentiment_stats: Dict) -> str:
        """API í‚¤ê°€ ì—†ì„ ë•Œì˜ ë”ë¯¸ ë¶„ì„"""
        return f"""
### ğŸ“Š ì—¬ë¡  ë¶„ì„ ìš”ì•½ (ë”ë¯¸ ë°ì´í„°)
'{query}'ì— ëŒ€í•œ ë¶„ì„ì´ ìš”ì²­ë˜ì—ˆìŠµë‹ˆë‹¤.

### ğŸ’­ ì£¼ìš” ì˜ê²¬ë“¤
í˜„ì¬ AI ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ê°ì • ë¶„ì„ ê²°ê³¼:
- ê¸ì •: {sentiment_stats['positive']:.1%}
- ë¶€ì •: {sentiment_stats['negative']:.1%}
- ì¤‘ë¦½: {sentiment_stats['neutral']:.1%}

### ğŸ¯ ê²°ë¡ 
ì‹¤ì œ ì‚¬ìš©ì„ ìœ„í•´ì„œëŠ” Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
"""
    
    def _generate_no_data_response(self, query: str) -> str:
        """ê´€ë ¨ ëŒ“ê¸€ì´ ì—†ì„ ë•Œì˜ ì‘ë‹µ"""
        return f"""
### âš ï¸ ë°ì´í„° ë¶€ì¡±
'{query}'ì™€ ê´€ë ¨ëœ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:
1. ê²€ìƒ‰ì–´ê°€ ì •í™•í•œì§€ í™•ì¸
2. í•´ë‹¹ ì£¼ì œì˜ ëŒ“ê¸€ì´ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
3. ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì‹œë„

ë¨¼ì € í•´ë‹¹ ì£¼ì œì˜ ìœ íŠœë¸Œ ëŒ“ê¸€ì„ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”.
"""
    
    async def generate_simple_response(
        self,
        query: str,
        context_documents: List[Dict],
        conversation_history: List[ChatMessage] = None
    ) -> str:
        """ê°„ë‹¨í•œ RAG ì‘ë‹µ ìƒì„±"""
        if not self.llm:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ í¬ë§·íŒ…
        context_text = ""
        if context_documents:
            context_text = "\n".join([
                f"- {doc.get('content', '')[:200]}..."
                for doc in context_documents[:3]
            ])
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_text = ""
        if conversation_history:
            history_text = "\n".join([
                f"{msg.role}: {msg.content}"
                for msg in conversation_history[-5:]  # ìµœê·¼ 5ê°œë§Œ
            ])
        
        prompt_text = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ê´€ë ¨ ì •ë³´:
{context_text}

ì´ì „ ëŒ€í™”:
{history_text}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        try:
            # ChatPromptTemplateì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ í˜•ì‹ ìˆ˜ì •
            from langchain.schema import HumanMessage
            messages = [HumanMessage(content=prompt_text)]
            response = await self.llm.agenerate([messages])
            return response.generations[0][0].text.strip()
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def generate_direct_response(
        self,
        query: str,
        conversation_history: List[ChatMessage] = None
    ) -> str:
        """RAG ì—†ì´ ì§ì ‘ ì‘ë‹µ ìƒì„±"""
        if not self.llm:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_text = ""
        if conversation_history:
            history_text = "\n".join([
                f"{msg.role}: {msg.content}"
                for msg in conversation_history[-5:]  # ìµœê·¼ 5ê°œë§Œ
            ])
        
        prompt_text = f"""ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•´ì£¼ì„¸ìš”.

ì´ì „ ëŒ€í™”:
{history_text}

ì‚¬ìš©ì: {query}

ì–´ì‹œìŠ¤í„´íŠ¸:"""
        
        try:
            # ChatPromptTemplateì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ í˜•ì‹ ìˆ˜ì •
            from langchain.schema import HumanMessage
            messages = [HumanMessage(content=prompt_text)]
            response = await self.llm.agenerate([messages])
            return response.generations[0][0].text.strip()
        except Exception as e:
            return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}" 